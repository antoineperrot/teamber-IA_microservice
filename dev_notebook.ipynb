{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99d2c8f4",
   "metadata": {},
   "source": [
    "## en-tête des requêtes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8fa4c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "api_url = 'https://development.api.wandeed.com/api/lst/search?offset=0&limit=500'\n",
    "access_token='Bearer eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJydXRUeHJ5dlltOUVZcGhpRjRxak45ajFvTktLNnU4YUhuM1QySFFSUU5FIn0.eyJleHAiOjE2NjMwNzExMzMsImlhdCI6MTY2MzA1MzEzMywiYXV0aF90aW1lIjoxNjYzMDUzMTMyLCJqdGkiOiJiYmViZDY1ZC00ODgwLTQ3MDUtOTdiOS1kNWNjYjFjOWFjZjIiLCJpc3MiOiJodHRwczovL2RldmVsb3BtZW50LmF1dGgud2FuZGVlZC5jb20vYXV0aC9yZWFsbXMvd2FuZGVlZC1yZWFsbSIsImF1ZCI6ImFjY291bnQiLCJzdWIiOiIzMWZmODY3NC1iOGZhLTQyMmYtYWM3NC02YzFjZGI2YTUwZGUiLCJ0eXAiOiJCZWFyZXIiLCJhenAiOiJ3YW5kZWVkLWNsaWVudCIsIm5vbmNlIjoiMDY1MjU1NmMtMjA2Ny00ZTA3LTg2N2ItZTU3YTc4ZGFmNGQxIiwic2Vzc2lvbl9zdGF0ZSI6IjJlNTNhMDJmLWQ0YTQtNDU5OS05YjRhLWI0YjZmMDM2MTY0YyIsImFjciI6IjEiLCJhbGxvd2VkLW9yaWdpbnMiOlsiaHR0cHM6Ly8qLndhbmRlZWQuY29tLyoiLCJodHRwczovLyouYXBpLndhbmRlZWQuY29tLyoiLCIqIiwiaHR0cHM6Ly8qLmFkbWluLndhbmRlZWQuY29tLyoiLCJodHRwczovLyouYXV0aC53YW5kZWVkLmNvbSJdLCJyZWFsbV9hY2Nlc3MiOnsicm9sZXMiOlsiZGVmYXVsdC1yb2xlcy13YW5kZWVkLXJlYWxtIiwib2ZmbGluZV9hY2Nlc3MiLCJ1bWFfYXV0aG9yaXphdGlvbiJdfSwicmVzb3VyY2VfYWNjZXNzIjp7ImFjY291bnQiOnsicm9sZXMiOlsibWFuYWdlLWFjY291bnQiLCJtYW5hZ2UtYWNjb3VudC1saW5rcyIsInZpZXctcHJvZmlsZSJdfX0sInNjb3BlIjoib3BlbmlkIGVtYWlsIHByb2ZpbGUiLCJzaWQiOiIyZTUzYTAyZi1kNGE0LTQ1OTktOWI0YS1iNGI2ZjAzNjE2NGMiLCJ1dGxfc3BrdXRpbGlzYXRldXIiOjEsInV0bF91dGlsaXNhdGV1cl9yb2xlcyI6IlsxXSIsInV0bF9jcHJlbm9tIjoiR2F5bG9yZCIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJ1dGxfc2FwcGFydGVuYW5jZSI6WzIsMzhdLCJ1dGxfY25vbSI6IlBldGl0IiwicHJlZmVycmVkX3VzZXJuYW1lIjoiZ2F5bG9yZC5wZXRpdEB0ZWFtYmVyLmZyIiwibG9jYWxlIjoiZnIiLCJnaXZlbl9uYW1lIjoiR2F5bG9yZCIsInV0bF9zZmtpbnN0YW5jZSI6MSwidXRsX3NhZG1pbmlzdHJlciI6W10sIm5hbWUiOiJHYXlsb3JkIFBldGl0IiwidXRsX3Nkcm9pdHNhY2NlcyI6WzYwLDMsNCw1LDE1LDI1LDI2LDMzLDM0LDM1LDM2LDM3LDQwLDQxLDQ0LDQ1LDQ2LDQ3XSwiZmFtaWx5X25hbWUiOiJQZXRpdCIsImVtYWlsIjoiZ2F5bG9yZC5wZXRpdEB0ZWFtYmVyLmZyIiwidXNlcl9ncm91cHMiOlsyLDM4LDYwLDMsNCw1LDE1LDI1LDI2LDMzLDM0LDM1LDM2LDM3LDQwLDQxLDQ0LDQ1LDQ2LDQ3XX0.DGvq43C3ZqBNi7-ZUmrHXm5_iV7nC5gAo-d4cRhdYU8KjsB9N6i4IKxPFPnbalHJpZe9g4MGiZdWGbVYnBstuNcT5ioQOrlZP1BWUmN8Nkq-yYrnL3uHf7VB8mixOAu0b3kpF7r2XoytCvslDFBGaDcWGPJkIdMNFHw3fMKT-smjj1DTP01RuSUc-ItGHn3-CBBDkEocpwTTfBTEDEwcFZ1n69ct1XjYQPuNvKp2PCoje3Y68h6tGSllhOp6ikksx7-inXvMZ6u9ucoshrAkBvb2PLTkQ6cEZxdqz-ymJQsx4N3pxp71-S0xAsSFM8l9JlQ4Ittkmz1ReDkEKi0Vpw'\n",
    "\n",
    "headers={'Authorization': f'{access_token}',\n",
    "             'Content-Type':'application/json'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14665683",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://127.0.0.1:8000/get_data_task_assigner'\n",
    "res = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f5d47b",
   "metadata": {},
   "source": [
    "### dict PROJET -> Liste Utilisateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "id": "69310d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data_from_request(res):\n",
    "    \"\"\"\n",
    "    RECUPERATION DES DONNEES\n",
    "    \"\"\"\n",
    "    \n",
    "    # recuperation matrice_projet\n",
    "    df_prj = pd.DataFrame(res.json()['matrice_projet']['data']['result'])\n",
    "    # recuperation matrice_competence : on oublie les compétences pour lesquels les utl sont indéfinis\n",
    "    df_cmp = pd.DataFrame(res.json()['matrice_competence']['data']['result']).dropna().reset_index(drop=True).astype(int)\n",
    "    # recuperation des taches à assigner :\n",
    "    df_tsk = pd.DataFrame(res.json()['taches']['data']['result'])\n",
    "    # recuperation des disponibilites utl :\n",
    "    df_dsp = pd.DataFrame(res.json()['dispos_utilisateurs']['data']['result'])\n",
    "    \n",
    "    return df_prj, df_cmp, df_tsk, df_dsp\n",
    "\n",
    "def make_list_ids(df_prj, df_cmp, df_tsk, df_dsp):\n",
    "    \"\"\"\n",
    "    FAIT LA LISTE DE TOUS LES IDS (PRJ, CMP, TSK, UTL) CONTENUS DANS LES DONNEES RECUES\n",
    "    \"\"\"\n",
    "    # sauvegarde des id utilisateurs ET conservation des ids_utl uniques\n",
    "    lst_utl = []\n",
    "    lst_utl.append(list(np.unique(df_prj['utl_spkutilisateur'])))\n",
    "    lst_utl.append(list(np.unique(df_cmp['emc_sfkutilisateur'])))\n",
    "    lst_utl.append(list(np.unique(df_dsp['utl_spkutilisateur'])))\n",
    "    id_utl = list(np.sort(np.unique(np.sum(lst_utl,dtype=object))))\n",
    "    \n",
    "    # sauvegarde des id projets ET conservation des ids_prj uniques\n",
    "    lst_prj = []\n",
    "    lst_prj.append(list(np.unique(df_prj['int_sfkprojet'])))\n",
    "    lst_prj.append(list(np.unique(df_tsk['evt_sfkprojet'])))\n",
    "    id_prj = list(np.sort(np.unique(np.sum(lst_prj,dtype=object))))\n",
    "    \n",
    "    # sauvegardes des ids competences ET conservation des ids_cmp uniques\n",
    "    lst_cmp = []\n",
    "    lst_cmp.append(list(np.unique(df_cmp['emc_sfkarticle'])))\n",
    "    lst_cmp.append(list(np.unique(df_tsk['lgl_sfkligneparent'])))\n",
    "    id_cmp = list(np.sort(np.unique(np.sum(lst_cmp,dtype=object))))\n",
    "    \n",
    "    # sauvegardes des ids tsk ET conservation des ids_tsk uniques\n",
    "    lst_tsk = []\n",
    "    id_tsk = list(np.sort(np.unique(df_tsk['evt_spkevenement'])))\n",
    "    \n",
    "    return id_utl, id_prj, id_cmp, id_tsk\n",
    "\n",
    "def make_mapping_dicts_extern_to_local(id_utl, id_prj, id_cmp, id_tsk):\n",
    "    \"\"\"\n",
    "    CONVERSION DES IDS Wandeed en Identifiant local\n",
    "    \"\"\"\n",
    "    utl_to_int = {int(_id) : int(_int) for _int,_id in enumerate(id_utl)}\n",
    "    prj_to_int = {int(_id) : int(_int) for _int,_id in enumerate(id_prj)}\n",
    "    cmp_to_int = {int(_id) : int(_int) for _int,_id in enumerate(id_cmp)}\n",
    "    tsk_to_int = {int(_id) : int(_int) for _int,_id in enumerate(id_tsk)}\n",
    "    return utl_to_int, prj_to_int, cmp_to_int, tsk_to_int\n",
    "\n",
    "def make_mapping_dicts_local_to_extern(id_utl, id_prj, id_cmp, id_tsk):\n",
    "    \"\"\"\n",
    "    CONVERSION DES Identifiant local en IDS Wandeed\n",
    "    \"\"\"\n",
    "    int_to_utl = {int(_int): int(_id)  for _int,_id in enumerate(id_utl)}\n",
    "    int_to_utl['not assigned'] = 'not assigned'\n",
    "    int_to_prj = {int(_int): int(_id)  for _int,_id in enumerate(id_prj)}\n",
    "    int_to_cmp = {int(_int): int(_id)  for _int,_id in enumerate(id_cmp)}\n",
    "    int_to_tsk = {int(_int): int(_id)  for _int,_id in enumerate(id_tsk)}\n",
    "    return int_to_utl, int_to_prj, int_to_cmp, int_to_tsk\n",
    "\n",
    "def add_local_ids_in_dfs(df_prj, df_cmp, df_tsk, df_dsp,\n",
    "                        cmp_to_int, utl_to_int, tsk_to_int, prj_to_int):\n",
    "    \"\"\"\n",
    "    AJOUT DES VARIABLES LOCALES DANS LES DATAFRAMES\n",
    "    \"\"\"\n",
    "    df_cmp['cmp'] = df_cmp['emc_sfkarticle'].map(cmp_to_int)\n",
    "    df_cmp['utl'] = df_cmp['emc_sfkutilisateur'].map(utl_to_int)\n",
    "    df_tsk['tsk'] = df_tsk['evt_spkevenement'].map(tsk_to_int)\n",
    "    df_tsk['cmp'] = df_tsk['lgl_sfkligneparent'].map(cmp_to_int)\n",
    "    df_tsk['prj'] = df_tsk['evt_sfkprojet'].map(prj_to_int)\n",
    "    df_prj['utl'] = df_prj['utl_spkutilisateur'].map(utl_to_int)\n",
    "    df_prj['prj'] = df_prj['int_sfkprojet'].map(prj_to_int)\n",
    "    df_dsp['utl'] = df_dsp['utl_spkutilisateur'].map(utl_to_int)\n",
    "\n",
    "    # commodités pour plus tard\n",
    "    df_cmp = df_cmp.sort_values(by='utl')\n",
    "    return df_prj, df_cmp, df_tsk, df_dsp\n",
    "\n",
    "def make_mat_prj(df_prj,n_prj, n_utl):\n",
    "    \"\"\"\n",
    "    FABRICATION MATRICE PROJET\n",
    "    \"\"\"\n",
    "    # construction d'un dictionnaire qui contient, pour chaque prjet, la liste des utilisateurs en faisant parti.\n",
    "    d_prj_to_utl = df_prj.groupby('prj')['utl'].apply(np.sort).to_dict()\n",
    "    \n",
    "    # REMPLISSAGE MATRICE PROJET :\n",
    "    mat_prj = np.zeros((n_prj, n_utl)).astype(int)\n",
    "    for prj in range(n_prj): \n",
    "        for utl in d_prj_to_utl[prj]:\n",
    "            mat_prj[prj, utl] = 1\n",
    "    \n",
    "    return mat_prj\n",
    "\n",
    "def make_mat_cmp(df_cmp, n_cmp, n_utl):\n",
    "    \"\"\"\n",
    "    FABRICATION MATRICE COMPETENCE\n",
    "    \"\"\"\n",
    "    \n",
    "    # construction matrice de cmp np.array\n",
    "    mat_cmp = np.zeros((n_cmp, n_utl)).astype(int)\n",
    "\n",
    "    # REMPLISSAGE MATRICE DE COMPETENCE\n",
    "\n",
    "    # ce dict est organisé en arborescence : utl//comp//niveau\n",
    "    d_utl_to_cmp_to_lvl = {}\n",
    "    utl_competants = list(np.unique(df_cmp['utl']))\n",
    "    for utl in utl_competants:\n",
    "        d_utl_to_cmp_to_lvl[utl] = {}\n",
    "        df_cmp_tmp = df_cmp.loc[df_cmp['utl']==utl,]\n",
    "        for i, row in df_cmp_tmp.iterrows():\n",
    "            cmp = row['cmp'] ; lvl = row['emc_sniveau'];\n",
    "            d_utl_to_cmp_to_lvl[utl][cmp] = lvl\n",
    "\n",
    "    for utl in d_utl_to_cmp_to_lvl:\n",
    "        for cmp in d_utl_to_cmp_to_lvl[utl].keys():\n",
    "            lvl = d_utl_to_cmp_to_lvl[utl][cmp]\n",
    "            mat_cmp[cmp, utl] = lvl\n",
    "    \n",
    "    return mat_cmp\n",
    "\n",
    "def make_usefull_mapping_dicts(df_tsk, df_dsp):\n",
    "    \"\"\"\n",
    "    FABRICATION DE DICTIONNAIRE UTILES PAR LA SUITE\n",
    "    \"\"\"\n",
    "    d_tsk_to_cmp = {int(row['tsk']):int(row['cmp']) for i, row in df_tsk.iterrows()}\n",
    "    d_tsk_to_prj = {int(row['tsk']):int(row['prj']) for i, row in df_tsk.iterrows()}\n",
    "    d_tsk_to_lgt = {int(row['tsk']):row['evt_dduree'] for i, row in df_tsk.iterrows()}\n",
    "    d_utl_to_dsp = {int(row['utl']):row['utl_sdispo'] for _, row in df_dsp.iterrows()}          \n",
    "    d_utl_to_dsp['not assigned'] = np.sum(list(d_utl_to_dsp.values()))\n",
    "    return d_tsk_to_cmp, d_tsk_to_prj, d_tsk_to_lgt, d_utl_to_dsp\n",
    "\n",
    "def make_arcs_and_cost_func(n_tsk, n_utl, \n",
    "              d_tsk_to_cmp, d_tsk_to_prj,\n",
    "              mat_cmp, mat_prj,\n",
    "              penalty=-100):\n",
    "    \"\"\"\n",
    "    FABRICATION DES ARCS RELIANT TACHES A UTILISATEURS POTENTIELS, AINSI QUE FONCTION DE COUT\n",
    "    \"\"\"\n",
    "    arcs = []\n",
    "    n_arcs = 0\n",
    "    cost_func = []\n",
    "    for tsk in range(n_tsk):\n",
    "        for utl in range(n_utl):\n",
    "            cmp = d_tsk_to_cmp[tsk]\n",
    "            prj = d_tsk_to_prj[tsk]\n",
    "            lvl = mat_cmp[cmp, utl]\n",
    "            utl_on_prj = mat_prj[prj, utl]\n",
    "            if lvl >= 0 and utl_on_prj : \n",
    "                arcs.append( tuple((tsk,utl)) )\n",
    "                cost_func.append(lvl)\n",
    "\n",
    "    # chaque tache a également la possibilité de ne pas être assignée, ce qui est fortement pénalisé\n",
    "    for tsk in range(n_tsk):\n",
    "        arcs.append(tuple((tsk,'not assigned')))\n",
    "        cost_func.append(penalty)\n",
    "\n",
    "    # ajout des variables df'écart (slack variables)\n",
    "    cost_func += [0]*n_utl  \n",
    "    cost_func = np.array(cost_func)\n",
    "    n_arcs = len(arcs)\n",
    "    return arcs, cost_func, n_arcs\n",
    "\n",
    "def make_A_and_b(n_tsk,n_utl,n_arcs,\n",
    "                 d_tsk_to_lgt, d_utl_to_dsp,\n",
    "                 arcs):\n",
    "    \n",
    "    \"\"\"\n",
    "    FABRICATION DES MATRICES A et B POUR RESOUDRE AX<=B\n",
    "    \"\"\"\n",
    "    \n",
    "    #equality constraints :\n",
    "    A = np.zeros((n_tsk + n_utl , n_arcs + n_utl ))\n",
    "    b = np.zeros(n_tsk + n_utl)\n",
    "\n",
    "    # contrainte d'égalité : distribution de toutes les heures\n",
    "    for tsk in range(n_tsk):\n",
    "        for idx_arc, arc in enumerate(arcs):\n",
    "            if arc[0] == tsk:  A[tsk, idx_arc] = 1\n",
    "        b[tsk] = d_tsk_to_lgt[tsk]\n",
    "\n",
    "    # contraintes d'inégalités : respect des disponibilités de travail\n",
    "    for utl in range(n_utl):\n",
    "        for j in range(n_arcs - n_tsk):\n",
    "            arc = arcs[j]\n",
    "            utl_2 = arc[1]\n",
    "            if utl_2 == utl : A[n_tsk + utl, j] = 1\n",
    "        A[n_tsk + utl, n_arcs + utl] =   1 # variable d'écart (slack variable)\n",
    "        b[n_tsk + utl] = d_utl_to_dsp[utl]\n",
    "        \n",
    "    return A, b\n",
    "\n",
    "def solve_linear_programmation_problem(A, b, cost_func):    \n",
    "    \"\"\"\n",
    "    RESOLUTION DU PROBLEME DE PROGRAMMATION LINEAIRE\n",
    "    \"\"\"\n",
    "    from scipy.optimize import linprog\n",
    "    method='simplex'\n",
    "    l = linprog(-cost_func,A_eq=A,b_eq=b,method=\"simplex\",options={'maxiter':1500})\n",
    "    if l.status !=0 :\n",
    "        l = linprog(-c, A_eq=A,b_eq=b,method='interior-point',options={'maxiter':1500})\n",
    "        outcome = l.message\n",
    "        method = 'interior-point'\n",
    "    else :\n",
    "        outcome = l.message\n",
    "\n",
    "    solution_vector = l.x\n",
    "    return solution_vector, outcome, method\n",
    "\n",
    "def make_output_dataframe(solution, arcs,\n",
    "                          d_tsk_to_lgt, d_tsk_to_cmp, d_tsk_to_prj):\n",
    "    \"\"\"\n",
    "    MET EN FORME LA SOLUTION DANS UN DATAFRAME PANDAS\n",
    "    \"\"\"\n",
    "    out = pd.DataFrame()\n",
    "    for j in range(len(arcs)):\n",
    "        if solution_vector[j] > 0 :\n",
    "            tsk, utl = arcs[j]\n",
    "            lvl = cost_func[j]\n",
    "            out = out.append(pd.DataFrame({\n",
    "                'tsk':[tsk],'utl':[utl],\n",
    "                'duree_assignee':[solution_vector[j]],\n",
    "                'tsk_lgt':[d_tsk_to_lgt[tsk]],\n",
    "                'duree_non_assignee':[d_tsk_to_lgt[tsk] - solution_vector[j]],\n",
    "                'dsp_utl':[d_utl_to_dsp[utl]],\n",
    "                'cmp':[d_tsk_to_cmp[tsk]],\n",
    "                'lvl':[lvl],\n",
    "                'prj':[d_tsk_to_prj[tsk]]\n",
    "            } ))\n",
    "    out.reset_index(drop=True,inplace=True)  \n",
    "    out.loc[out['utl']=='not assigned','lvl'] = None\n",
    "    return out\n",
    "\n",
    "def remap_df_out(df_out,\n",
    "                int_to_tsk, int_to_utl, int_to_prj, int_to_cmp):\n",
    "    \"\"\"\n",
    "    REMAPPING DES ID LOCAUX EN ID WANDEED\n",
    "    \"\"\"\n",
    "    df_out['tsk'] = df_out['tsk'].map(int_to_tsk)\n",
    "    df_out['utl'] = df_out['utl'].map(int_to_utl)\n",
    "    df_out['prj'] = df_out['prj'].map(int_to_prj)\n",
    "    df_out['cmp'] = df_out['cmp'].map(int_to_cmp)\n",
    "    df_out['duree_assignee'] = np.round(df_out['duree_assignee'],2)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def make_stat_cmp(df_out):\n",
    "    \"\"\"\n",
    "    Production de statistiques par compétences\n",
    "    \"\"\"\n",
    "    avg_lvl = df_out.loc[df_out['utl']!='not assigned'].groupby('cmp')['lvl'].mean().rename('niveau_cmp_moyen_par_h_realisee')\n",
    "    cmp_miss = df_out.loc[df_out['utl']=='not assigned'].groupby('cmp')['duree_assignee'].sum().rename('total_h_non_assignee')\n",
    "    stat_cmp = pd.DataFrame([avg_lvl, cmp_miss]).T\n",
    "    stat_cmp['total_h_non_assignee'] = stat_cmp['total_h_non_assignee'].fillna(0)\n",
    "    stat_cmp.reset_index(inplace=True)\n",
    "    return stat_cmp\n",
    "\n",
    "def make_stat_utl(df_out, d_utl_to_dsp, utl_to_int):\n",
    "    \"\"\"\n",
    "    Production de statistiques par utilisateur\n",
    "    \"\"\"\n",
    "    avg_lvl = df_out[['utl','duree_assignee','lvl']].loc[df_out['utl']!='not assigned']\n",
    "    avg_lvl['niveau_moyen_execution_tsk'] = avg_lvl['duree_assignee']*avg_lvl['lvl']\n",
    "    avg_lvl_exe_tsk = np.round(avg_lvl.groupby('utl')['niveau_moyen_execution_tsk'].sum()/avg_lvl.groupby('utl')['duree_assignee'].sum(),1)\n",
    "    avg_lvl_exe_tsk.rename('niveau_moyen_execution_tsk',inplace=True)\n",
    "    tot_h = df_out.loc[df_out['utl']!='not assigned'].groupby('utl')['duree_assignee'].sum().rename('total_h_assignees')\n",
    "    stat_utl = pd.DataFrame([avg_lvl_exe_tsk, tot_h]).T\n",
    "    stat_utl['utl_int'] = stat_utl.index.map(utl_to_int)\n",
    "    stat_utl['dsp_utl'] = stat_utl.utl_int.map(d_utl_to_dsp)\n",
    "    stat_utl['taux_occupation'] = np.round(stat_utl['total_h_assignees'] / stat_utl['dsp_utl'], 2)\n",
    "    stat_utl = stat_utl.reset_index().drop('utl_int',axis=1)\n",
    "    return stat_utl\n",
    "\n",
    "def make_stat_tsk(df_out, d_tsk_to_lgt, int_to_tsk):\n",
    "    \"\"\"\n",
    "    Production de statistiques par tache\n",
    "    \"\"\"\n",
    "    n_utl_per_tsk = df_out.loc[df_out['utl']!='not assigned'].groupby('tsk')['utl'].count().rename('n_utl_per_tsk').astype(int)\n",
    "    tmp = pd.Series(d_tsk_to_lgt).sort_index()\n",
    "    tmp.set_axis(list(int_to_tsk.values()),inplace=True)\n",
    "    pct_per_tsk = ((df_out.loc[df_out['utl']!='not assigned'].groupby('tsk')['duree_assignee'].sum()/tmp).fillna(0).rename('pct_assignation_tache')*100).astype(int)\n",
    "    stat_tsk = pd.DataFrame([n_utl_per_tsk, pct_per_tsk]).T\n",
    "    stat_tsk['n_utl_per_tsk'] =  stat_tsk['n_utl_per_tsk'].fillna(0).astype(int)\n",
    "    stat_tsk['pct_assignation_tache'] =  stat_tsk['pct_assignation_tache'].astype(int)\n",
    "    stat_tsk.reset_index(inplace=True)\n",
    "    stat_tsk.rename(mapper={\"index\":'utl'},axis=1,inplace=True)\n",
    "    return stat_tsk\n",
    "\n",
    "def make_stat_prj(df_out):\n",
    "    \"\"\"\n",
    "    Production de statistiques par projet\n",
    "    \"\"\"\n",
    "    unassigned_time_per_prj = df_out.groupby('prj')['duree_non_assignee'].sum().rename('temps_total_non_assigne')\n",
    "    n_missing_cmp_per_prj = df_out.loc[df_out['utl']=='not assigned'].groupby('prj')['cmp'].count().rename('n_missing_cmp_per_prj')\n",
    "    stat_prj = pd.DataFrame([unassigned_time_per_prj, n_missing_cmp_per_prj]).T\n",
    "    stat_prj.reset_index(inplace=True)\n",
    "    stat_prj['n_missing_cmp_per_prj'] =  stat_prj['n_missing_cmp_per_prj'].fillna(0).astype(int)\n",
    "    \n",
    "    return stat_prj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "id": "555bfc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antoi\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "# import librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## RECUPERATION DES DONNEES A PARTIR DE LA REQUETE :\n",
    "df_prj, df_cmp, df_tsk, df_dsp = collect_data_from_request(res)\n",
    "\n",
    "# FAIT LA LISTE DE TOUS LES IDS (PRJ, CMP, TSK, UTL) CONTENUS DANS LES DONNEES RECUES\n",
    "id_utl, id_prj, id_cmp, id_tsk = make_list_ids(df_prj, df_cmp, df_tsk, df_dsp)\n",
    "\n",
    "# CONVERSION DES IDS Wandeed en Identifiant local\n",
    "utl_to_int, prj_to_int, cmp_to_int, tsk_to_int = make_mapping_dicts_extern_to_local(id_utl, id_prj, id_cmp, id_tsk)\n",
    "\n",
    "# CONVERSION DES Identifiant local en IDS Wandeed\n",
    "int_to_utl, int_to_prj, int_to_cmp, int_to_tsk = make_mapping_dicts_local_to_extern(id_utl, id_prj, id_cmp, id_tsk)\n",
    "\n",
    "# Comptage nombre cmp, utl, tsk, prj.\n",
    "n_utl = len(id_utl); n_prj = len(id_prj); n_cmp = len(id_cmp); n_tsk = len(id_tsk);\n",
    "\n",
    "\n",
    "# AJOUT DES VARIABLES LOCALES DANS LES DATAFRAMES\n",
    "df_prj, df_cmp, df_tsk, df_dsp = add_local_ids_in_dfs(df_prj, df_cmp, df_tsk, df_dsp,\n",
    "                                                     cmp_to_int, utl_to_int, tsk_to_int, prj_to_int)\n",
    "\n",
    "# FABRICATION MATRICE PROJET\n",
    "mat_prj = make_mat_prj(df_prj,n_prj, n_utl)\n",
    "\n",
    "# FABRICATION MATRICE COMPETENCE\n",
    "mat_cmp = make_mat_cmp(df_cmp, n_cmp, n_utl)\n",
    "        \n",
    "## MOCKING mat_cmp : ######################################################################\n",
    "mat_cmp = np.random.binomial(n=3,p=0.25, size=mat_cmp.shape)\n",
    "\n",
    "## MOCKING mat_prj : ######################################################################\n",
    "mat_prj = np.random.binomial(n=1,p=0.25, size=mat_prj.shape)\n",
    "\n",
    "        \n",
    "# FABRICATION DE DICTIONNAIRE UTILES PAR LA SUITE\n",
    "d_tsk_to_cmp, d_tsk_to_prj, d_tsk_to_lgt, d_utl_to_dsp = make_usefull_mapping_dicts(df_tsk, df_dsp)\n",
    "\n",
    "# MOCKING pour les dsp utl#################################################################\n",
    "d_utl_to_dsp = {utl:np.random.randint(15,50)*0.1 for utl in range(n_utl)}\n",
    "d_utl_to_dsp['not assigned'] = np.sum(list(d_utl_to_dsp.values()))\n",
    "\n",
    "# FABRICATION DES ARCS RELIANT TACHES A UTILISATEURS POTENTIELS, AINSI QUE FONCTION DE COUT\n",
    "arcs, cost_func, n_arcs = make_arcs_and_cost_func(n_tsk, n_utl, \n",
    "              d_tsk_to_cmp, d_tsk_to_prj,\n",
    "              mat_cmp, mat_prj)\n",
    "\n",
    "# FABRICATION DES MATRICES A et B POUR RESOUDRE AX<=B\n",
    "A, b = make_A_and_b(n_tsk,n_utl,n_arcs,\n",
    "                 d_tsk_to_lgt, d_utl_to_dsp,\n",
    "                 arcs)\n",
    "\n",
    "# RESOLUTION DU PROBLEME DE PROGRAMMATION LINEAIRE\n",
    "solution_vector, outcome, method = solve_linear_programmation_problem(A, b, cost_func)\n",
    "\n",
    "\n",
    "# MISE EN FORME DE LA SOLUTION DANS UN DATAFRAME PANDAS\n",
    "df_out = make_output_dataframe(solution, arcs,\n",
    "                          d_tsk_to_lgt, d_tsk_to_cmp, d_tsk_to_prj)\n",
    "\n",
    "# REMAPPING DES ID LOCAUX EN ID WANDEED\n",
    "df_out = remap_df_out(df_out,\n",
    "                int_to_tsk, int_to_utl, int_to_prj, int_to_cmp)\n",
    "\n",
    "## PRODUCTION DE STATISTIQUES D'INTERPRETATION DU RESULTAT\n",
    "\n",
    "# Production de statistiques par compétences\n",
    "stat_cmp = make_stat_cmp(df_out)\n",
    "\n",
    "# Production de statistiques par utilisateur\n",
    "stat_utl = make_stat_utl(df_out, d_utl_to_dsp, utl_to_int)\n",
    "\n",
    "# Production de statistiques par tache\n",
    "stat_tsk = make_stat_tsk(df_out, d_tsk_to_lgt, int_to_tsk)\n",
    "\n",
    "# Production de statistiques par projet\n",
    "stat_prj = make_stat_prj(df_out)\n",
    "\n",
    "# SORTIE API\n",
    "OUT = {'solution':df_out.to_dict(),\n",
    "      'statistics_for':{\n",
    "          'cmp':stat_cmp.to_dict(),\n",
    "          'utl':stat_utl.to_dict(),\n",
    "          'tsk':stat_tsk.to_dict(),\n",
    "          'prj':stat_prj.to_dict()\n",
    "      }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "id": "4aaae880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mapping(df_out):\n",
    "    assert all(df_out.tsk.apply(lambda x: x in id_tsk)), \"Certains id_tsk ne sont pas dans la liste des id_tsk fournis au départ.\"\n",
    "    assert all(df_out.prj.apply(lambda x: x in id_prj)), \"Certains id_prj ne sont pas dans la liste des id_prj fournis au départ.\"\n",
    "    assert all(df_out.cmp.apply(lambda x: x in id_cmp)), \"Certains id_cmp ne sont pas dans la liste des id_cmp fournis au départ.\"\n",
    "    assert all(df_out.utl.apply(lambda x: x in id_utl +['not assigned'])), \"Certains id_utl ne sont pas dans la liste des id_utl fournis au départ.\"\n",
    "test_mapping(df_out)\n",
    "\n",
    "def test_validite_mathematique_solution(df_out, solution_vector):\n",
    "    assert all(A @ solution_vector <= b + 1e-10), \"Math : la solution_vector x ne respecte pas la contrainte Ax <= b.\"  # + 1e-10 : prise en compte d'une tolérance d'erreur d'arrondi numérique.\n",
    "    assert all(df_out['duree_assignee'] >=0), \"Un nombre d'heure négatif a été assigné.\"\n",
    "    assert all(df_out['duree_assignee'] <= df_out['tsk_lgt']), \"La durée assignée pour certaines tâches excède celle leur durée.\"\n",
    "test_validite_mathematique_solution(df_out, solution_vector)    \n",
    "\n",
    "def test_stat_cmp(stat_cmp):\n",
    "    assert all(stat_cmp['total_h_non_assignee'] >=0), \"ValueError dans total_h_non_assignee\"\n",
    "    assert all(stat_cmp[~stat_cmp['niveau_cmp_moyen_par_h_realisee'].isna()]['niveau_cmp_moyen_par_h_realisee'] <= 3),\"Mauvais calcul du niveau moyen d'exécution dune tache : > 3.\"\n",
    "    assert all(stat_cmp[~stat_cmp['niveau_cmp_moyen_par_h_realisee'].isna()]['niveau_cmp_moyen_par_h_realisee'] >= 0),\"Mauvais calcul du niveau moyen d'exécution dune tache : < 0.\"\n",
    "\n",
    "test_stat_cmp(stat_cmp)\n",
    "\n",
    "def test_stat_utl(stat_utl):\n",
    "    assert all(stat_utl.taux_occupation.apply(pd.notna)), \"TypeError: les données de taux d'occupation contiennent des NaN.\"\n",
    "    assert all(stat_utl.taux_occupation.apply(lambda x: (isinstance(x, float) or isinstance(x, int)) and x >= 0 and x <= 1 ) ), \"TypeError: les données de taux d'occupation sont incorrectes.\"\n",
    "    assert all(stat_utl.dsp_utl.apply(pd.notna)), \"TypeError: les données de disponibilités totales contiennent des NaN.\"\n",
    "    assert all(stat_utl.dsp_utl.apply(lambda x: (isinstance(x, float) or isinstance(x, int)) and x >= 0 ) ), \"TypeError: les données de disponibilités totales sont incorrectes.\"\n",
    "    assert all(stat_utl.dsp_utl.apply(lambda x: x >= 0 )), \"TypeError: Certaines données de disponibilités totales sont négatives.\"\n",
    "    assert all(stat_utl['total_h_assignees'] <= stat_utl['dsp_utl']  ), \"Certains utilisateurs sont trop chargés par rapport à leurs disponibiltés.\"\n",
    "    assert all(stat_utl['niveau_moyen_execution_tsk'] <= 3), \"Le calcul du niveau moyen d'exécution d'une tache par unité de temps est FAUX.\"\n",
    "    assert all(stat_utl['niveau_moyen_execution_tsk'] >= 0), \"Le calcul du niveau moyen d'exécution d'une tache par unité de temps est FAUX.\"\n",
    "    \n",
    "test_stat_utl(stat_utl)\n",
    "\n",
    "def test_stat_prj(stat_prj):\n",
    "    assert all(stat_prj.temps_total_non_assigne\t >=0), \"ValueError dans temps_total_non_assigne\"\n",
    "    assert all(stat_prj.n_missing_cmp_per_prj.apply(lambda x: isinstance(x,int))), \"ValueError: Le nombre de compétences manquantes par projet n'est pas toujours un entier.\"\n",
    "\n",
    "test_stat_prj(stat_prj)\n",
    "\n",
    "def test_stat_tsk(stat_tsk):\n",
    "    assert all(stat_tsk.n_utl_per_tsk.apply(lambda x: isinstance(x,int))), \"ValueError: Le nombre d'utilisateurs par tache n'est pas toujours un entier.\"\n",
    "\n",
    "test_stat_tsk(stat_tsk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
